from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.mongo.hooks.mongo import MongoHook
from datetime import datetime, timedelta
import subprocess
import os
import json
import psutil
import csv
from pymongo import MongoClient
from airflow.models import Variable
import pandas as pd
import time
import os
from threading import Thread

SCRIPT_MODULE = "scripts.Nouvelalgo"
CORPUS_PATH = "scripts/corpus2.json"
EXTRACTION_DIR = "/usr/local/airflow/"
METRICS_CSV = "/usr/local/airflow/metrics.csv"

default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}


def monitoring(pid, results):
    process = psutil.Process(pid)
    print(f"Monitoring process {pid}...")
    cpu_set = set()
    mem_max = 0
    mem_max_system = 0
    cpu_percent_max_process = 0
    cpu_percent_max_system = 0
    while results["running"]:
        try:
            mem = process.memory_info().rss / 1024 ** 2  # en MB
            mem_max = max(mem_max, mem)

            mem_system = psutil.virtual_memory().used / 1024 ** 2  # en MB
            mem_max_system = max(mem_max_system, mem_system)

            cpu_set.update(process.cpu_affinity())  

            cpu_percent_process = process.cpu_percent(interval=0.1)
            cpu_percent_system = psutil.cpu_percent(interval=0.1)
        
            cpu_percent_max_process = max(cpu_percent_max_process, cpu_percent_process)
            cpu_percent_max_system = max(cpu_percent_max_system, cpu_percent_system)



            time.sleep(0.1)
        except psutil.NoSuchProcess:
            break

    results["cpu_percent_process"] = cpu_percent_max_process
    results["cpu_percent_system"] = cpu_percent_max_system
    results["mem_max_process"] = mem_max
    results["mem_max_system"] = mem_max_system

def append_metrics_to_csv(metrics, path=METRICS_CSV):
    file_exists = os.path.isfile(path)
    with open(path, "a", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=metrics.keys())
        if not file_exists:
            writer.writeheader()
        writer.writerow(metrics)


def initialize_data(**kwargs):
    """
    T√¢che Airflow : Ex√©cute le script de collecte initialisante des donn√©es via un sous-processus et enregistre les m√©triques.

    - Lance un thread de monitoring de la consommation m√©moire pendant l'ex√©cution du script.
    - Ex√©cute un script Python avec des arguments li√©s √† la p√©riode d'extraction des tweets 25 juin 2026 au 1 janvier 2006 (cr√©ation de Twitter).
    - Mesure la dur√©e d‚Äôex√©cution et envoie le r√©sultat dans XCom :
        * `start_time` : heure de d√©but
        * `extract_time` : dur√©e totale d‚Äôex√©cution
        * `status` : succ√®s ou √©chec
    - Affiche la sortie standard et les erreurs si le script √©choue.
    """

    # Enregistre l'heure de d√©but d'ex√©cution de la t√¢che
    start_time = datetime.now()

    # R√©cup√®re l'identifiant du processus courant (utile pour le monitoring)
    pid = os.getpid()
    print("üîç Starting monitoring extract_data ...")

    # D√©marre un thread pour surveiller la consommation m√©moire du processus
    results = {"running": True}  # Drapeau de contr√¥le pour arr√™ter le thread ensuite
    monitor_thread = Thread(target=monitoring, args=(pid, results))
    monitor_thread.start()

    # D√©finit le d√©but et la fin de la p√©riode d'extraction
    today = "2025-06-26"
    # Calcule la date d‚Äôhier √† la m√™me heure (10h30)
    day_ago = "2006-01-01"

    # Pr√©pare la commande √† ex√©cuter pour lancer le script Python de scraping
    cmd = [
        "python", "-m", SCRIPT_MODULE,      # Lancement du module Python √† ex√©cuter
        CORPUS_PATH,                        # Chemin vers le corpus
        "10:30", today,    # Heure et date de fin d‚Äôextraction
        "10:30", day_ago,  # Heure et date de d√©but d‚Äôextraction
        "bs4", "0", "--finrecolte"          # Param√®tres suppl√©mentaires : m√©thode bs4, index, etc.
    ]

    # Affiche la commande dans les logs Airflow
    print("Executing command:", " ".join(cmd))

    # Ex√©cute le script en sous-processus et capture la sortie
    result = subprocess.run(cmd, capture_output=True)
    print("result:", result)

    # Enregistre l‚Äôheure de fin
    end_time = datetime.now()

    # Envoie l'heure de d√©but et la dur√©e d'ex√©cution dans les XComs (pour analyse ou logs Airflow)
    kwargs["ti"].xcom_push(key="start_time", value=start_time.isoformat())
    kwargs["ti"].xcom_push(key="extract_time", value=(end_time - start_time).total_seconds())

    # V√©rifie si l‚Äôex√©cution s‚Äôest bien pass√©e (code de retour 0)
    if result.returncode == 0:
        status = "success"
    else:
        status = "failed"
        print("‚ùå Script failed:")
        print("STDOUT:", result.stdout)  # Affiche ce que le script a retourn√©
        print("STDERR:", result.stderr)  # Affiche les erreurs √©ventuelles

    # Envoie le statut (succ√®s/√©chec) dans XCom pour d‚Äôautres t√¢ches √©ventuelles
    kwargs["ti"].xcom_push(key="status", value=status)


    results["running"] = False  # arr√™ter la surveillance
    monitor_thread.join()

    # R√©sultats
    print("\n‚úÖ Monitoring termin√© extract_data.")
    print(f"pourcentage max CPU du processus : {results['cpu_percent_process']:.2f}%")
    print(f"pourcentage max CPU du syst√®me : {results['cpu_percent_system']:.2f}%")
    print(f"m√©moire max du processus : {results ['mem_max_process']:.2f} MB")
    print(f"m√©moire max du syst√®me : {results['mem_max_system']:.2f} MB")

    kwargs["ti"].xcom_push(key="pourcent_cpu_process", value=results['cpu_percent_process'])
    kwargs["ti"].xcom_push(key="pourcent_cpu_system", value=results['cpu_percent_system'])
    kwargs["ti"].xcom_push(key="mem_max_process", value=results['mem_max_process'])
    kwargs["ti"].xcom_push(key="mem_max_system", value=results['mem_max_system'])


def preprocess_single(tweet):
    """
    Pr√©-traite un tweet individuel pour enrichir les informations temporelles et pr√©parer la structure.

    Args:
        tweet (dict): Un dictionnaire repr√©sentant un tweet brut avec ses attributs.

    Returns:
        dict: Un dictionnaire enrichi contenant les m√™mes champs que le tweet original, 
              plus des champs d√©riv√©s de la date (ann√©e, mois, jour, heure, minute, seconde),
              et un champ `date_tweet_cleaned` au format datetime string.
    """
    # Conversion de la date en objet datetime pandas pour faciliter les extractions
    date_datetime = pd.to_datetime(tweet['date_tweet'], format='%Y-%m-%dT%H:%M:%S.%fZ')

    # Renvoi d‚Äôun nouveau dictionnaire enrichi
    return {
        "text_tweet": tweet['text_tweet'],
        "cleaned_text_tweet": "",  # vide ici, √† remplir apr√®s nettoyage
        "nombre_likes": tweet['nombre_likes'],
        "nombre_reposts": tweet['nombre_reposts'],
        "nombre_replies": tweet['nombre_replies'],
        "nombre_views": tweet['nombre_views'],
        "date_tweet": tweet['date_tweet'],
        "identifiant": tweet['identifiant'],
        "req_id": tweet['req_id'],
        "mot_cle": tweet['mot_cle'],
        "bool_analyse": tweet['bool_analyse'],
        "emotion": tweet['emotion'],
        "date_tweet_cleaned": str(date_datetime),  # date sous forme string
        'year': int(date_datetime.year),           # extraction de l'ann√©e
        'month': int(date_datetime.month),         # extraction du mois
        'day': int(date_datetime.day),             # extraction du jour
        'hour': int(date_datetime.hour),           # extraction de l'heure
        'minute': int(date_datetime.minute),       # extraction des minutes
        'second': int(date_datetime.second),       # extraction des secondes
    }

def transform_data(**kwargs):
    
    """
    Fonction Airflow pour transformer les donn√©es brutes extraites.

    Args:
        **kwargs: Param√®tres de contexte pass√©s automatiquement par Airflow.

    Returns:
        list: Liste des tweets transform√©s (pr√©-trait√©s) ou liste vide si aucun tweet.

    Fonctionnalit√©s :
    - Charge les tweets JSON bruts extraits.
    - Applique la fonction `preprocess_single` √† chaque tweet.
    - Surveille en parall√®le l'utilisation CPU et m√©moire pendant la transformation.
    - Envoie des m√©triques (nombre de tweets, dur√©e, consommation CPU/m√©moire) dans XCom.
    - Affiche les statistiques de monitoring √† la fin.
    """
     
    start_transform = datetime.now()  # heure de d√©but
    pid = os.getpid()
    print("üîç Starting monitoring transform data ...")

    # D√©marrage du monitoring CPU/m√©moire dans un thread s√©par√©
    results = {"running": True}
    monitor_thread = Thread(target=monitoring, args=(pid, results))
    monitor_thread.start()

    # Chargement des tweets bruts depuis un fichier JSON
    raw_path = os.path.join(EXTRACTION_DIR, "tweets_raw.json")
    with open(raw_path, "r", encoding="utf-8") as f:
        tweets = json.load(f)

    if tweets:
        # Pr√©-traitement de chaque tweet
        cleaned = [preprocess_single(t) for t in tweets]
        # Push dans XCom le nombre de tweets trait√©s
        kwargs["ti"].xcom_push(key="n_records", value=len(cleaned))
        # Push dans XCom la dur√©e totale de transformation
        kwargs["ti"].xcom_push(key="transform_time", value=(datetime.now() - start_transform).total_seconds())
        # Arr√™t du monitoring
        results["running"] = False
        monitor_thread.join()

        # Affichage des r√©sultats de monitoring
        # R√©sultats
        print("\n‚úÖ Monitoring termin√© extract_data.")
        print(f"pourcentage max CPU du processus : {results['cpu_percent_process']:.2f}%")
        print(f"pourcentage max CPU du syst√®me : {results['cpu_percent_system']:.2f}%")
        print(f"m√©moire max du processus : {results ['mem_max_process']:.2f} MB")
        print(f"m√©moire max du syst√®me : {results['mem_max_system']:.2f} MB")
        kwargs["ti"].xcom_push(key="pourcent_cpu_process", value=results['cpu_percent_process'])
        kwargs["ti"].xcom_push(key="pourcent_cpu_system", value=results['cpu_percent_system'])
        kwargs["ti"].xcom_push(key="mem_max_process", value=results['mem_max_process'])
        kwargs["ti"].xcom_push(key="mem_max_system", value=results['mem_max_system'])
        return cleaned
    else:
        print("Aucun tweet √† traiter.")
        kwargs["ti"].xcom_push(key="n_records", value=0)
        kwargs["ti"].xcom_push(key="transform_time", value=(datetime.now() - start_transform).total_seconds())
        results["running"] = False  # arr√™ter la surveillance
        monitor_thread.join()

        # R√©sultats
        print("\n‚úÖ Monitoring termin√© transform data.")
        print(f"pourcentage max CPU du processus : {results['cpu_percent_process']:.2f}%")
        print(f"pourcentage max CPU du syst√®me : {results['cpu_percent_system']:.2f}%")
        print(f"m√©moire max du processus : {results ['mem_max_process']:.2f} MB")
        print(f"m√©moire max du syst√®me : {results['mem_max_system']:.2f} MB")
        kwargs["ti"].xcom_push(key="pourcent_cpu_process", value=results['cpu_percent_process'])
        kwargs["ti"].xcom_push(key="pourcent_cpu_system", value=results['cpu_percent_system'])
        kwargs["ti"].xcom_push(key="mem_max_process", value=results['mem_max_process'])
        kwargs["ti"].xcom_push(key="mem_max_system", value=results['mem_max_system'])
        return []

def insert_into_mongo(**kwargs):
    """
    Ins√®re les donn√©es nettoy√©es dans la base MongoDB et enregistre la requ√™te.

    Fonction utilis√©e dans un contexte Airflow, qui r√©cup√®re les donn√©es transform√©es 
    via XCom, convertit les dates, puis ins√®re les tweets et la requ√™te dans la base MongoDB.
    Parall√®lement, un monitoring de consommation CPU/m√©moire est lanc√©.

    Args:
        **kwargs: Param√®tres de contexte Airflow contenant notamment 'ti' pour acc√©der aux XCom.

    Fonctionnalit√©s :
    - D√©marre un thread pour monitorer l‚Äôusage CPU/m√©moire du processus.
    - R√©cup√®re les donn√©es nettoy√©es de la t√¢che pr√©c√©dente ('transform_data') via XCom.
    - Convertit la date de chaque tweet en datetime (format pandas).
    - Connexion √† MongoDB via un hook Airflow.
    - Ins√®re les donn√©es dans la collection 'tweets'.
    - Ins√®re les informations de la requ√™te dans la collection 'requests'.
    - Arr√™te le monitoring √† la fin.
    """

    start_insert = datetime.now()  # Temps de d√©but de l'insertion

    pid = os.getpid()  # ID du processus actuel
    print("üîç Starting monitoring insert data ...")

    # D√©marrer un thread de monitoring CPU/m√©moire en parall√®le
    results = {"running": True}
    monitor_thread = Thread(target=monitoring, args=(pid, results))
    monitor_thread.start()

    # R√©cup√©rer le contexte Airflow pour acc√©der aux donn√©es via XCom
    ti = kwargs['ti']
    cleaned_data = ti.xcom_pull(task_ids='transform_data')  # Donn√©es transform√©es
    hook = MongoHook(mongo_conn_id='mongo_id_audensiel')
    db = hook.get_conn().Recolte  # Acc√®s √† la base Recolte
    if not cleaned_data:
        print("Aucune donn√©e √† ins√©rer dans MongoDB.")
    else : 
    # Conversion des dates des tweets en objets datetime pandas
        for tweet in cleaned_data:
            tweet['date_tweet_cleaned'] = pd.to_datetime(tweet['date_tweet'], format='%Y-%m-%dT%H:%M:%S.%fZ')
        db.tweets.insert_many(cleaned_data)
    
    # Chargement des informations de requ√™te depuis un fichier JSON
    req_path = os.path.join(EXTRACTION_DIR, "req.json")
    with open(req_path, "r", encoding="utf-8") as f:
        req_data = json.load(f)

    # Insertion de la requ√™te dans la collection 'requests'
    db.requests.insert_one(req_data)

    # Chargement des informations de requ√™te depuis un fichier JSON
    req_path = os.path.join(EXTRACTION_DIR, "req.json")
    with open(req_path, "r", encoding="utf-8") as f:
        req_data = json.load(f)

    # Insertion de la requ√™te dans la collection 'requests'
    db.requests.insert_one(req_data)

    end_time = datetime.now()  # Temps de fin de l'insertion

    # Arr√™ter le monitoring CPU/m√©moire
    results["running"] = False
    monitor_thread.join()

    print("\n‚úÖ Monitoring termin√© insert data.")
    print(f"pourcentage max CPU du processus : {results['cpu_percent_process']:.2f}%")
    print(f"pourcentage max CPU du syst√®me : {results['cpu_percent_system']:.2f}%")
    print(f"m√©moire max du processus : {results ['mem_max_process']:.2f} MB")
    print(f"m√©moire max du syst√®me : {results['mem_max_system']:.2f} MB")
    
    start_time_str = ti.xcom_pull(task_ids="extract_data", key="start_time")
    start_time = datetime.fromisoformat(start_time_str)
    
    latency_s = (end_time - start_time).total_seconds()
    duration_s = (end_time - start_insert).total_seconds()
    n_records = ti.xcom_pull(task_ids="transform_data", key="n_records")
    throughput = n_records / latency_s if latency_s > 0 else 0
    extract_s = ti.xcom_pull(task_ids="extract_data", key="extract_time")
    transform_s = ti.xcom_pull(task_ids="transform_data", key="transform_time")

    #memoire et cpu
    #extract
    extract_cpu_process = ti.xcom_pull(task_ids="initialize", key="pourcent_cpu_process")
    extract_cpu_system = ti.xcom_pull(task_ids="initialize", key="pourcent_cpu_system")
    extract_mem_process = ti.xcom_pull(task_ids="initialize", key="mem_max_process")
    extract_mem_system = ti.xcom_pull(task_ids="initialize", key="mem_max_system")

    transform_cpu_process = ti.xcom_pull(task_ids="transform_data", key="pourcent_cpu_process")
    transform_cpu_system = ti.xcom_pull(task_ids="transform_data", key="pourcent_cpu_system")
    transform_mem_process = ti.xcom_pull(task_ids="transform_data", key="mem_max_process")
    transform_mem_system = ti.xcom_pull(task_ids="transform_data", key="mem_max_system")

    insert_cpu_process = results['cpu_percent_process']
    insert_cpu_system = results['cpu_percent_system']
    insert_mem_process = results['mem_max_process']
    insert_mem_system = results['mem_max_system']

    pipeline_cpu_process_max = max(extract_cpu_process, transform_cpu_process, insert_cpu_process)
    pipeline_cpu_system_max = max(extract_cpu_system, transform_cpu_system, insert_cpu_system)
    pipeline_mem_process_max = max(extract_mem_process, transform_mem_process, insert_mem_process)
    pipeline_mem_system_max = max(extract_mem_system, transform_mem_system, insert_mem_system)

    metrics = {
        "n_records": n_records,
        "latency_s": latency_s,
        "throughput_rps": throughput,
        "insert_time_s": duration_s,
        "transform_time_s" : transform_s,
        "extract_time_s" : extract_s,
        "cpu_percent_process": pipeline_cpu_process_max,
        "cpu_percent_system": pipeline_cpu_system_max,
        "mem_max_process": pipeline_mem_process_max,
        "mem_max_system": pipeline_mem_system_max,
    }
    

    append_metrics_to_csv(metrics)
    ti.xcom_push(key="status", value="success")


def compute_failure_rate(**kwargs):
    ti = kwargs['ti']
    statuses = [
        ti.xcom_pull(task_ids="extract_data", key="status"),
        ti.xcom_pull(task_ids="transform_data", key="status"),
        ti.xcom_pull(task_ids="insert_mongo", key="status")
    ]
    failure_rate = statuses.count("fail") / len(statuses)
    print(f"[FAILURE RATE] : {failure_rate*100:.2f}%")


# D√©finition du DAG quotidien pour le pipeline ETL Twitter
with DAG(
    dag_id="twitter_etl_init_dag",               # Identifiant unique du DAG
    default_args=default_args,                     # Arguments par d√©faut (retries, owner, etc.)
    start_date=datetime(2025, 5, 26, 8, 0),       # Date et heure de d√©marrage du DAG
    schedule_interval=None,               # Planification : tous les jours √† 10h30
    catchup=False,                                 # Pas d‚Äôex√©cution r√©troactive des dates manqu√©es
    tags=["manuel"]                               # Tag pour cat√©goriser ce DAG dans l‚Äôinterface Airflow
) as daily_dag:

    # T√¢che d‚Äôextraction des donn√©es Twitter
    initialize = PythonOperator(
        task_id="extract_data",
        python_callable=initialize_data,
        provide_context=True  # Permet de passer le contexte Airflow √† la fonction
    )
    # T√¢che de transformation des donn√©es extraites
    transform = PythonOperator(
        task_id="transform_data",
        python_callable=transform_data
    )

    # T√¢che d‚Äôinsertion des donn√©es transform√©es dans MongoDB
    insert = PythonOperator(
        task_id="insert_mongo",
        python_callable=insert_into_mongo
    )

    # T√¢che pour calculer le taux d‚Äô√©chec (monitoring ou alerting)
    failure = PythonOperator(
        task_id="compute_failure",
        python_callable=compute_failure_rate
    )

    # Ordonnancement des t√¢ches : initialisation ‚Üí transformation ‚Üí insertion ‚Üí calcul des √©checs
    initialize >> transform >> insert >> failure
